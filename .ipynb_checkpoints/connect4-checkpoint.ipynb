{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "7rjkZl7XYZRZ"
   },
   "source": [
    "The following notebook creates tic tac toe players using a q-learning reinforcement learning strategy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "OziQ6ZIF_URy",
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "import random\n",
    "import matplotlib \n",
    "import numpy as np \n",
    "from collections import defaultdict "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "tAEp-Jot_1qZ",
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "class BoardEnvironment:\n",
    "  \"\"\" this class creates an environment for agents to interact with\"\"\"\n",
    "\n",
    "  def __init__(self):\n",
    "    \"initialize board\"\n",
    "  \n",
    "  def set_players(self, playerA, playerB):\n",
    "    \" connects players with the environment \"\n",
    "    self.playerA = playerA\n",
    "    self.playerB = playerB\n",
    "    self.reset() # defines current_player\n",
    "      \n",
    "  def reset(self):\n",
    "    self.turn = 'X' # the board always starts with X, regardless of which player\n",
    "\n",
    "    # board states are a 42-character representing the state of the board.\n",
    "    self.board = list('-------------------------')  \n",
    "    if (self.playerA and self.playerB): # if they are set\n",
    "      self.playerA.reset_past()\n",
    "      self.playerB.reset_past()\n",
    "      if (random.random() < 0.5):  # randomly pick the player to start\n",
    "        self.current_player = self.playerA\n",
    "      else:\n",
    "        self.current_player = self.playerB \n",
    "\n",
    "  def print_board(self, board_string = None):\n",
    "    \"print more readable board either from supplied board string or the current board\"\n",
    "    if not board_string:\n",
    "      B = self.board\n",
    "    else:\n",
    "      B = board_string\n",
    "    print(B[0],'|', B[1],'|', B[2],'|',B[3],'|',B[4], sep='')\n",
    "    # print('-------------')\n",
    "    print(B[5],'|', B[6],'|', B[7],'|',B[8],'|',B[9], sep='')\n",
    "    # print('-------------')\n",
    "    print(B[10],'|', B[11],'|', B[12],'|',B[13],'|',B[14], sep='')\n",
    "    # print('-------------')\n",
    "    print(B[15],'|', B[16],'|', B[17],'|',B[18],'|',B[19], sep='')\n",
    "    # print('-------------')\n",
    "    print(B[20],'|', B[21],'|', B[22],'|',B[23],'|',B[24], sep='')\n",
    "\n",
    "  def get_state(self):\n",
    "    return \"\".join(self.board)\n",
    "  \n",
    "  def other_player(self):\n",
    "    # note, returns other player even if playerA is playing itself\n",
    "    if (self.current_player == self.playerA):\n",
    "      return self.playerB \n",
    "    else:\n",
    "      return self.playerA  \n",
    "    \n",
    "  def available_actions(self):\n",
    "    return [ind for ind, val in enumerate(self.board) if val == '-']\n",
    "\n",
    "  def play_game(self): \n",
    "    # returns the winning player or None if a tie\n",
    "    self.reset()\n",
    "    while (not self.is_full() ):\n",
    "      choice = self.current_player.select_action()\n",
    "\n",
    "      self.board[choice] = self.turn # should check if valid\n",
    "\n",
    "      if self.winner(self.turn):\n",
    "        self.current_player.reward(100)\n",
    "        self.other_player().reward(-100)\n",
    "        return self.current_player\n",
    "      else: # no one has won yet\n",
    "        self.other_player().reward(0)\n",
    "\n",
    "      # switch players\n",
    "      self.turn = 'X' if self.turn == 'O' else 'O' # switch turn\n",
    "      self.current_player = self.other_player()\n",
    "    # it's a tie  \n",
    "    return None\n",
    "        \n",
    "  def winner(self, turn):\n",
    "    straight_lines = (\n",
    "\t\t\t(0,1,2,3),\n",
    "\t\t\t(1,2,3,4),\n",
    "            (5,6,7,8),\n",
    "            (6,7,8,9),\n",
    "            (10,11,12,13),\n",
    "            (11,12,13,14),\n",
    "            (15,16,17,18),\n",
    "            (16,17,18,19),\n",
    "            (20,21,22,23),\n",
    "            (21,22,23,24),\n",
    "\n",
    "            (0,6,12,18),\n",
    "            (6,12,18,24),\n",
    "            (1,7,13,19),\n",
    "            (5,11,17,23),\n",
    "            \n",
    "\n",
    "            (3,7,11,15),\n",
    "            (4,8,12,16),\n",
    "            (8,12,16,20),\n",
    "            (9,13,17,21),\n",
    "\n",
    "\t\t\t(0,5,10,15),\n",
    "            (5,10,15,20),\n",
    "            (1,6,11,16),\n",
    "            (6,11,16,21),\n",
    "            (2,7,12,17),\n",
    "            (7,12,17,22),\n",
    "            (3,8,13,18),\n",
    "            (8,13,18,23),\n",
    "            (4,9,14,19),\n",
    "            (9,14,19,24)\n",
    "\t\t\t)\n",
    "#     for turn in check_for:\n",
    "    for line in straight_lines:\n",
    "        if all(x == turn for x in (self.board[i] for i in line)):\n",
    "            return turn\n",
    "    return '' # if there is no winner\n",
    "\n",
    "  def is_full(self):\n",
    "    return('-' not in self.board) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "sXhti1q4a56Y",
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "class Agent:\n",
    "    \"\"\" this class is a generic Q-Learning reinforcement learning agent for discrete states and fixed actions\n",
    "    represented as strings\"\"\"\n",
    "    def __init__(self, environment, policy = 'max', learning_rate = 0.5, discount_factor = 0.95, epsilon = 0.1):\n",
    "        if policy in ['max', 'random', 'epsilon']:\n",
    "          self.policy = policy\n",
    "        else:\n",
    "          raise InputError(policy, ' is not an available policy')\n",
    "        self.learning_rate = learning_rate\n",
    "        self.discount_factor = discount_factor\n",
    "        self.Q = defaultdict(lambda: 0.0) # stores (state, action) value tuples as keys\n",
    "        self.environment = environment\n",
    "        self.epsilon = epsilon # Fraction of time making a random choice for epsilon policy\n",
    "        self.reset_past()\n",
    "\n",
    "    def reset_past(self):\n",
    "      self.past_action = None\n",
    "      self.past_state = None\n",
    "          \n",
    "    def select_action(self):\n",
    "      available_actions = self.environment.available_actions()\n",
    "      if (self.policy == 'random') or (self.policy == 'epsilon' and random.random() < self.epsilon):\n",
    "        choice = random.choice(available_actions)\n",
    "      else: #self.policy == 'max' or it's an epsilon policy determined to pick the max\n",
    "        Q_vals = [self.Q[(self.environment.get_state(), x)] for x in available_actions]\n",
    "        #randomly pick one of the maximum values\n",
    "        max_val = max(Q_vals) # will often be 0 in the beginning\n",
    "        max_pos = [i for i, j in enumerate(Q_vals) if j == max_val]\n",
    "        max_indices = [available_actions[x] for x in max_pos]\n",
    "        choice = random.choice(max_indices)\n",
    "      self.past_state = self.environment.get_state()\n",
    "      self.past_action = choice\n",
    "      return choice\n",
    "        \n",
    "    def reward(self, reward_value):\n",
    "        # finding the best expected reward\n",
    "        available_actions = self.environment.available_actions()\n",
    "        next_Q_vals = [self.Q[(self.environment.get_state(), x)] for x in available_actions]\n",
    "        max_next_Q = max(next_Q_vals) if next_Q_vals else 0 # will often be 0 in the beginning\n",
    "        td_target = reward_value + self.discount_factor * max_next_Q\n",
    "        reward_pred_error = td_target - self.Q[(self.past_state,self.past_action)]\n",
    "        if (self.past_state or self.past_action):\n",
    "          self.Q[(self.past_state,self.past_action)] += self.learning_rate * reward_pred_error\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "FR0NfH9ta56e",
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "class RepeatedGames:\n",
    "    def __init__(self, environment, playerA, playerB):\n",
    "        self.environment = environment\n",
    "        self.playerA = playerA\n",
    "        self.playerB = playerB\n",
    "        self.reset_history()\n",
    "    \n",
    "    def reset_history(self):\n",
    "        self.history = []\n",
    "    \n",
    "    def play_game(self):\n",
    "        winner = self.environment.play_game()\n",
    "        if (winner == self.playerA):\n",
    "          self.history.append('A')\n",
    "        elif (winner == self.playerB):\n",
    "          self.history.append('B')\n",
    "        else:\n",
    "          self.history.append('-')\n",
    "    \n",
    "    def play_games(self, games_to_play):\n",
    "        for i in range(games_to_play):\n",
    "            self.play_game()\n",
    "        print(self.history[-games_to_play:].count('A'),'games won by player A')\n",
    "        print(self.history[-games_to_play:].count('B'),'games won by player B')\n",
    "        print(self.history[-games_to_play:].count('-'),'ties')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 206
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 2234,
     "status": "ok",
     "timestamp": 1574367305000,
     "user": {
      "displayName": "Mark Albert",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AAuE7mDMI-vX2rHqg_XFX0NQHcU8Foh6IFf25z4_5Un6YYs=s64",
      "userId": "02522742638773023035"
     },
     "user_tz": 360
    },
    "id": "RoUxiTLVa56g",
    "outputId": "de61cdc9-0912-4d6f-98b9-2090388ecebd",
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "34 games won by player A\n",
      "56 games won by player B\n",
      "10 ties\n",
      "\n"
     ]
    }
   ],
   "source": [
    "board = BoardEnvironment()\n",
    "A = Agent(board, 'max')\n",
    "B = Agent(board, 'random')\n",
    "board.set_players(A,B)\n",
    "\n",
    "tournament = RepeatedGames(board,A,B)\n",
    "tournament.play_games(100)\n",
    "print()\n",
    "tournament.play_games(100000)\n",
    "print()\n",
    "tournament.play_games(100000)\n",
    "print()\n",
    "tournament.play_games(100000)\n",
    "print()\n",
    "tournament.play_games(100000)\n",
    "print()\n",
    "\n",
    "tournament.play_games(100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 497
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 373,
     "status": "ok",
     "timestamp": 1574367340029,
     "user": {
      "displayName": "Mark Albert",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AAuE7mDMI-vX2rHqg_XFX0NQHcU8Foh6IFf25z4_5Un6YYs=s64",
      "userId": "02522742638773023035"
     },
     "user_tz": 360
    },
    "id": "k3991koUa56j",
    "outputId": "4d512c96-868b-4e4a-ff3a-ceea1f31f800",
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "# observe the highest and lowest board state action value functions\n",
    "key_max = max(A.Q.keys(), key=(lambda k: A.Q[k]))\n",
    "print(\"highest Q for player A:\", A.Q[key_max],', state_action:', key_max)\n",
    "board.print_board(key_max[0])\n",
    "key_max = max(B.Q.keys(), key=(lambda k: B.Q[k]))\n",
    "print(\"\\nhighest Q for player B:\", B.Q[key_max],', state_action:', key_max)\n",
    "board.print_board(key_max[0])\n",
    "key_min = min(A.Q.keys(), key=(lambda k: A.Q[k]))\n",
    "print(\"\\nlowest Q for player A:\", A.Q[key_min],', state_action:', key_min)\n",
    "board.print_board(key_min[0])\n",
    "key_min = min(B.Q.keys(), key=(lambda k: B.Q[k]))\n",
    "print(\"\\nlowest Q for player B:\", B.Q[key_min],', state_action:', key_min)\n",
    "board.print_board(key_min[0])\n",
    "\n",
    "print(A.Q[('XXOOXXO--', 7)], A.Q[('XXOOXXO--', 8)])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 295
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 612,
     "status": "ok",
     "timestamp": 1574367399103,
     "user": {
      "displayName": "Mark Albert",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AAuE7mDMI-vX2rHqg_XFX0NQHcU8Foh6IFf25z4_5Un6YYs=s64",
      "userId": "02522742638773023035"
     },
     "user_tz": 360
    },
    "id": "Jkri0k5ga56l",
    "outputId": "7385bedd-3d60-4689-d711-3ea8444958f6",
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "# plot the history\n",
    "import numpy as np\n",
    "import pylab as py\n",
    "import matplotlib\n",
    "%matplotlib inline \n",
    "\n",
    "history = np.array(tournament.history.copy())\n",
    "rewards = np.zeros(len(history))\n",
    "rewards[history == 'A'] = 100\n",
    "rewards[history == 'B'] = -100\n",
    "\n",
    "def running_mean(x, N):\n",
    "    return np.convolve(x, np.ones((N,))/N, mode='valid')\n",
    "r_mean = running_mean(rewards, 100)\n",
    "py.plot(r_mean)\n",
    "py.xlabel('games played')\n",
    "py.ylabel('average reward')\n",
    "py.title('Average rewards over 100 games (win/loss is 100/-100)');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "XtmqktqjSKDu",
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "class LeagueEnvironment:\n",
    "  \"\"\" this creates a league for players to decide if they want to play \"\"\"\n",
    "  def __init__(self, board_environment):\n",
    "    self.board = board_environment    \n",
    "\n",
    "  def set_players(self, player_names, league_agents, board_agents):\n",
    "    self.player_names = player_names\n",
    "    self.league_agents = league_agents\n",
    "    self.board_agents = board_agents\n",
    "    assert(len(player_names) == len(league_agents) == len(board_agents) )\n",
    "    self.num_players = len(player_names)\n",
    "\n",
    "  def reset_pair(self):\n",
    "    # randomly select 2 players\n",
    "    player_indices = list(range(self.num_players))\n",
    "    self.Ai = random.choice(player_indices)\n",
    "    player_indices.remove(self.Ai)\n",
    "    self.Bi = random.choice(player_indices)\n",
    "\n",
    "    self.board.set_players(self.board_agents[self.Ai], self.board_agents[self.Bi])\n",
    "    self.league_agents[self.Ai].reset_past()\n",
    "    self.league_agents[self.Bi].reset_past()\n",
    "    self.A_wins = 0;\n",
    "    self.ties = 0;\n",
    "    self.B_wins = 0;\n",
    "    self.state_perspective = 'A' # the state in wins/ties/losses for which player\n",
    "\n",
    "  def get_state(self):  ### how to tell who is calling get_state?\n",
    "    if self.state_perspective == 'A':\n",
    "      return (self.A_wins, self.ties, self.B_wins)\n",
    "    else: # show for player B\n",
    "      return (self.B_wins, self.ties, self.A_wins)\n",
    "\n",
    "  def pair_games_played(self):\n",
    "    return self.A_wins + self.ties + self.B_wins  \n",
    "\n",
    "  def available_actions(self):\n",
    "    if self.pair_games_played() < 10:\n",
    "      return ['play','quit']\n",
    "    else:\n",
    "      return ['quit']\n",
    "\n",
    "  def play_pair(self): \n",
    "    # returns the winning player or None if a tie\n",
    "\n",
    "    self.reset_pair() # picks a new random pair of players\n",
    "\n",
    "    while (True):\n",
    "\n",
    "      self.state_perspective = 'A'\n",
    "      A_choice = self.league_agents[self.Ai].select_action()\n",
    "      self.state_perspective = 'B'\n",
    "      B_choice = self.league_agents[self.Bi].select_action()\n",
    "\n",
    "      # is it over? then return with the overall winner\n",
    "      if A_choice == 'quit' or B_choice == 'quit':\n",
    "        break;\n",
    "\n",
    "      # now play a game    \n",
    "      winner = self.board.play_game()\n",
    "\n",
    "      # record the winner\n",
    "      if winner == self.board_agents[self.Ai]:\n",
    "        self.A_wins += 1\n",
    "      elif winner == self.board_agents[self.Bi]:\n",
    "        self.B_wins += 1\n",
    "      else:\n",
    "        self.ties += 1\n",
    "\n",
    "      if self.pair_games_played() == 10:\n",
    "        break;\n",
    "\n",
    "      # if it's not over, learn\n",
    "      self.state_perspective = 'A'\n",
    "      self.league_agents[self.Ai].reward(0)\n",
    "      self.state_perspective = 'B'         \n",
    "      self.league_agents[self.Bi].reward(0)\n",
    "\n",
    "    # when the match is over\n",
    "    self.ties += 0.1 # a hack to make a new end state different from the last one\n",
    "\n",
    "    reward_value = 100    # try self.pair_games_played() * 100?\n",
    "    if self.A_wins > self.B_wins:\n",
    "      self.state_perspective = 'A'\n",
    "      self.league_agents[self.Ai].reward(reward_value)\n",
    "      self.state_perspective = 'B'         \n",
    "      # self.league_agents[self.Bi].reward(- reward_value)\n",
    "      return (self.player_names[self.Ai], reward_value, self.player_names[self.Bi])\n",
    "    elif self.A_wins < self.B_wins:\n",
    "      self.state_perspective = 'A'\n",
    "      # self.league_agents[self.Ai].reward(- reward_value)\n",
    "      self.state_perspective = 'B'         \n",
    "      self.league_agents[self.Bi].reward(reward_value)\n",
    "      return (self.player_names[self.Bi], reward_value, self.player_names[self.Ai])\n",
    "    else: #same number of wins for A and B\n",
    "      self.state_perspective = 'A'\n",
    "      self.league_agents[self.Ai].reward(0)\n",
    "      self.state_perspective = 'B'         \n",
    "      self.league_agents[self.Bi].reward(0)\n",
    "      return (self.player_names[self.Ai],0,self.player_names[self.Bi])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 103
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 7921,
     "status": "ok",
     "timestamp": 1574367953600,
     "user": {
      "displayName": "Mark Albert",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AAuE7mDMI-vX2rHqg_XFX0NQHcU8Foh6IFf25z4_5Un6YYs=s64",
      "userId": "02522742638773023035"
     },
     "user_tz": 360
    },
    "id": "dYpRUVvYa56o",
    "outputId": "77ad0fcb-829b-4010-eb41-ac0765e2a513",
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "board = BoardEnvironment()\n",
    "league = LeagueEnvironment(board)\n",
    "\n",
    "player_names = []\n",
    "board_agents = []\n",
    "league_agents = []\n",
    "\n",
    "player_names.append('learning strategy and tactics')\n",
    "board_agents.append(Agent(board, 'max'))\n",
    "league_agents.append(Agent(league, 'max'))\n",
    "\n",
    "player_names.append('learning tactics only')\n",
    "board_agents.append(Agent(board, 'max'))\n",
    "league_agents.append(Agent(league, 'random'))\n",
    "\n",
    "player_names.append('learning strategy only')\n",
    "board_agents.append(Agent(board, 'random'))\n",
    "league_agents.append(Agent(league, 'max'))\n",
    "\n",
    "player_names.append('no learning')\n",
    "board_agents.append(Agent(board, 'random'))\n",
    "league_agents.append(Agent(league, 'random'))\n",
    "\n",
    "league.set_players(player_names, league_agents, board_agents)\n",
    "\n",
    "history_winner = []\n",
    "history_reward = []\n",
    "history_loser = []\n",
    "\n",
    "pairs_to_play = 30000\n",
    "for game in range(pairs_to_play):    \n",
    "    (winner, reward, loser) = league.play_pair()\n",
    "    history_winner.append(winner)\n",
    "    history_reward.append(reward)\n",
    "    history_loser.append(loser)\n",
    "\n",
    "history_winner = np.array(history_winner)\n",
    "history_reward = np.array(history_reward)\n",
    "history_loser = np.array(history_loser)\n",
    "\n",
    "print('results of first',pairs_to_play,'matches')\n",
    "for i in range(league.num_players):\n",
    "  matches_won = np.count_nonzero(np.array(history_reward > 0) & np.array(history_winner == player_names[i]) )\n",
    "  print(matches_won,'matches won by player',player_names[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 223
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 497,
     "status": "ok",
     "timestamp": 1574367954109,
     "user": {
      "displayName": "Mark Albert",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AAuE7mDMI-vX2rHqg_XFX0NQHcU8Foh6IFf25z4_5Un6YYs=s64",
      "userId": "02522742638773023035"
     },
     "user_tz": 360
    },
    "id": "xMEvMqWfZ5bT",
    "outputId": "9adbbd59-3b20-4c23-8ad3-f5837b4980fe",
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "# observe the highest and lowest board state action value functions\n",
    "for pi in range(league.num_players):\n",
    "  LA = league_agents[pi]\n",
    "  key_max = max(LA.Q.keys(), key=(lambda k: LA.Q[k]))\n",
    "  print(\"\\nhighest Q for\", player_names[pi], \":\", LA.Q[key_max],', state_action:', key_max)\n",
    "  key_min = min(LA.Q.keys(), key=(lambda k: LA.Q[k]))\n",
    "  print(\"lowest Q for\", player_names[pi], \":\", LA.Q[key_min],', state_action:', key_min)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 573
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 1736,
     "status": "ok",
     "timestamp": 1574367956596,
     "user": {
      "displayName": "Mark Albert",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AAuE7mDMI-vX2rHqg_XFX0NQHcU8Foh6IFf25z4_5Un6YYs=s64",
      "userId": "02522742638773023035"
     },
     "user_tz": 360
    },
    "id": "Zeo_34F3STop",
    "outputId": "dbca41cb-178f-43f9-8a89-0633d01e00bd",
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "# plot the history\n",
    "import numpy as np\n",
    "import pylab as py\n",
    "import matplotlib\n",
    "%matplotlib inline \n",
    "\n",
    "def running_mean(x, N):\n",
    "    return np.convolve(x, np.ones((N,))/N, mode='full')\n",
    "\n",
    "# plot the average winnings\n",
    "for pi in range(league.num_players):\n",
    "  rewards = history_reward.copy()\n",
    "  rewards[history_winner != player_names[pi]] = 0\n",
    "  r_mean = running_mean(rewards, 2000)\n",
    "  py.plot(r_mean[0:len(history_winner)]);\n",
    "\n",
    "py.xlabel('Matches played')\n",
    "py.ylabel('Average winnings')\n",
    "py.title( 'Average winnings over ' + str(len(history_winner)) + ' matches (win is 100)');\n",
    "py.legend(player_names, loc='upper right', framealpha=0.99);\n",
    "\n",
    "# plot the match win percentage for each pair\n",
    "py.figure()\n",
    "pair_labels = []\n",
    "for ai in range(league.num_players - 1):\n",
    "  for bi in range(ai+1,league.num_players):\n",
    "    a_vs_b_reward = np.zeros(history_reward.shape)\n",
    "    pair_labels.append(player_names[ai] + ' vs ' + player_names[bi])\n",
    "    for i in range(len(a_vs_b_reward)):\n",
    "      if history_reward[i] > 0 and history_winner[i] == player_names[ai] and history_loser[i] == player_names[bi]:\n",
    "        a_vs_b_reward[i] = 1\n",
    "      if history_reward[i] > 0 and history_winner[i] == player_names[bi] and history_loser[i] == player_names[ai]:\n",
    "        a_vs_b_reward[i] = -1\n",
    "    r_mean = running_mean(a_vs_b_reward, 2000)\n",
    "    py.plot(6 * r_mean[0:len(a_vs_b_reward)]);\n",
    "\n",
    "py.xlabel('Matches played')\n",
    "py.ylabel('Average wins - losses')\n",
    "py.title( 'Average value of wins-loss of A over B only when A vs B');\n",
    "py.legend(pair_labels, loc='upper right', framealpha=0.8);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "colab": {
   "collapsed_sections": [],
   "name": "tictactoe_betting.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  },
  "pycharm": {
   "stem_cell": {
    "cell_type": "raw",
    "metadata": {
     "collapsed": false
    },
    "source": []
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
